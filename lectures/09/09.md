---
title: "Large language model details"
layout: single
author_profile: true
author: Erik Rodner
classes: wide
---

**Task:** Please study the following content - you will dive into the details of a transformer layer and how
it works.
{: .notice--warning} 

| Topic | Content | 
| :------------- |  :---------- |
| [Attention layer](/modules/attention_layer/attention_layer.md) | How does attention work? |
| [Transformer architecture](/modules/transformer_architecture/transformer_architecture.md) | Constructing a transformer layer using the attention mechanism |


## ðŸ›  Workshop in the lecture

* Implementing dot-product attention